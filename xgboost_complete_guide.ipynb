{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Gradient Boosting - Complete Guide\n",
        "\n",
        "<!--\n",
        "Project: XGBoost Gradient Boosting\n",
        "Author: Molla Samser (Founder)\n",
        "Designer & Tester: Rima Khatun\n",
        "Website: https://rskworld.in\n",
        "Email: help@rskworld.in, support@rskworld.in\n",
        "Phone: +91 93305 39277\n",
        "Address: Nutanhat, Mongolkote, Purba Burdwan, West Bengal, India, 713147\n",
        "GitHub: https://github.com/rskworld\n",
        "-->\n",
        "\n",
        "This comprehensive guide demonstrates advanced XGBoost techniques including:\n",
        "- Model training and evaluation\n",
        "- Hyperparameter tuning\n",
        "- Cross-validation\n",
        "- Feature importance analysis\n",
        "- Model interpretation with SHAP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Designer & Tester: Rima Khatun\n",
        "# Website: https://rskworld.in\n",
        "# Email: help@rskworld.in, support@rskworld.in\n",
        "# Phone: +91 93305 39277\n",
        "# Address: Nutanhat, Mongolkote, Purba Burdwan, West Bengal, India, 713147\n",
        "# GitHub: https://github.com/rskworld\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# XGBoost and sklearn\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "# LabelEncoder available in utils/data_loader.py if needed\n",
        "\n",
        "# Model interpretation\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "    print(\"SHAP not available. Install with: pip install shap\")\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"XGBoost version: {xgb.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "We'll create synthetic datasets for both classification and regression tasks to demonstrate XGBoost capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Designer & Tester: Rima Khatun\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# Create classification dataset\n",
        "X_class, y_class = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create regression dataset\n",
        "X_reg, y_reg = make_regression(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    noise=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Convert to DataFrames for better visualization\n",
        "feature_names = [f'feature_{i+1}' for i in range(20)]\n",
        "df_class = pd.DataFrame(X_class, columns=feature_names)\n",
        "df_class['target'] = y_class\n",
        "\n",
        "df_reg = pd.DataFrame(X_reg, columns=feature_names)\n",
        "df_reg['target'] = y_reg\n",
        "\n",
        "print(\"Classification Dataset:\")\n",
        "print(df_class.head())\n",
        "print(f\"\\nShape: {df_class.shape}\")\n",
        "print(f\"Target distribution:\\n{df_class['target'].value_counts()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\\nRegression Dataset:\")\n",
        "print(df_reg.head())\n",
        "print(f\"\\nShape: {df_reg.shape}\")\n",
        "print(f\"Target statistics:\\n{df_reg['target'].describe()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Basic XGBoost Classification Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# Split classification data\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
        "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "# Create and train XGBoost classifier\n",
        "xgb_classifier = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_classifier.fit(\n",
        "    X_train_class, y_train_class,\n",
        "    eval_set=[(X_test_class, y_test_class)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_class = xgb_classifier.predict(X_test_class)\n",
        "y_pred_proba = xgb_classifier.predict_proba(X_test_class)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_class, y_pred_class))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Basic XGBoost Regression Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# Split regression data\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train XGBoost regressor\n",
        "xgb_regressor = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    eval_metric='rmse'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_regressor.fit(\n",
        "    X_train_reg, y_train_reg,\n",
        "    eval_set=[(X_test_reg, y_test_reg)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_reg = xgb_regressor.predict(X_test_reg)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Tuning with GridSearchCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Create base model\n",
        "xgb_base = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_base,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train_class, y_train_class)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best Parameters:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"\\nBest Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test_class, y_test_class)\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Cross-Validation Techniques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create model\n",
        "xgb_cv = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(\n",
        "    xgb_cv, X_class, y_class,\n",
        "    cv=kfold,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Cross-Validation Results:\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"Individual Scores: {cv_scores}\")\n",
        "print(f\"Min Score: {cv_scores.min():.4f}\")\n",
        "print(f\"Max Score: {cv_scores.max():.4f}\")\n",
        "\n",
        "# Visualize CV scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(cv_scores) + 1), cv_scores, 'o-', linewidth=2, markersize=8)\n",
        "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'Mean: {cv_scores.mean():.4f}')\n",
        "plt.fill_between(range(1, len(cv_scores) + 1),\n",
        "                 cv_scores.mean() - cv_scores.std(),\n",
        "                 cv_scores.mean() + cv_scores.std(),\n",
        "                 alpha=0.2, color='gray')\n",
        "plt.xlabel('Fold', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.title('K-Fold Cross-Validation Scores', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Importance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# Train model for feature importance\n",
        "xgb_importance = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_importance.fit(X_train_class, y_train_class)\n",
        "\n",
        "# Get feature importance\n",
        "importance_gain = xgb_importance.feature_importances_\n",
        "importance_dict = dict(zip(feature_names, importance_gain))\n",
        "importance_sorted = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Create DataFrame\n",
        "importance_df = pd.DataFrame(importance_sorted, columns=['Feature', 'Importance'])\n",
        "importance_df = importance_df.sort_values('Importance', ascending=True)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
        "plt.xlabel('Importance (Gain)', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.title('XGBoost Feature Importance', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 10 Most Important Features:\")\n",
        "print(importance_df.tail(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Interpretation with SHAP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "if SHAP_AVAILABLE:\n",
        "    # Create SHAP explainer\n",
        "    explainer = shap.TreeExplainer(xgb_importance)\n",
        "    \n",
        "    # Calculate SHAP values for a sample\n",
        "    shap_values = explainer.shap_values(X_test_class[:100])\n",
        "    \n",
        "    # Summary plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.summary_plot(shap_values, X_test_class[:100], feature_names=feature_names, show=False)\n",
        "    plt.title('SHAP Summary Plot - Feature Impact on Model Output', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Feature importance plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.summary_plot(shap_values, X_test_class[:100], feature_names=feature_names, \n",
        "                     plot_type=\"bar\", show=False)\n",
        "    plt.title('SHAP Feature Importance', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"SHAP analysis completed successfully!\")\n",
        "else:\n",
        "    print(\"SHAP is not installed. Install with: pip install shap\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Learning Curves and Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# Get evaluation results\n",
        "results = xgb_classifier.evals_result()\n",
        "epochs = len(results['validation_0']['logloss'])\n",
        "x_axis = range(0, epochs)\n",
        "\n",
        "# Plot learning curves\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
        "ax.plot(x_axis, results['validation_0']['logloss'], label='Test')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Epochs', fontsize=12)\n",
        "ax.set_ylabel('Log Loss', fontsize=12)\n",
        "ax.set_title('XGBoost Learning Curve', fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_class, y_pred_class)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Advanced Techniques\n",
        "\n",
        "### Early Stopping\n",
        "XGBoost supports early stopping to prevent overfitting by monitoring validation performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# Model with early stopping\n",
        "xgb_early_stop = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    n_estimators=1000,  # Set high, early stopping will control\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# Train with early stopping\n",
        "xgb_early_stop.fit(\n",
        "    X_train_class, y_train_class,\n",
        "    eval_set=[(X_test_class, y_test_class)],\n",
        "    early_stopping_rounds=10,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(f\"Best iteration: {xgb_early_stop.best_iteration}\")\n",
        "print(f\"Best score: {xgb_early_stop.best_score:.4f}\")\n",
        "\n",
        "# Evaluate\n",
        "early_stop_pred = xgb_early_stop.predict(X_test_class)\n",
        "early_stop_accuracy = accuracy_score(y_test_class, early_stop_pred)\n",
        "print(f\"Test Accuracy with Early Stopping: {early_stop_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Persistence\n",
        "\n",
        "Save and load trained models for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# Save model\n",
        "xgb_classifier.save_model('xgboost_model.json')\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# Load model\n",
        "loaded_model = xgb.XGBClassifier()\n",
        "loaded_model.load_model('xgboost_model.json')\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Verify loaded model\n",
        "loaded_pred = loaded_model.predict(X_test_class)\n",
        "loaded_accuracy = accuracy_score(y_test_class, loaded_pred)\n",
        "print(f\"Loaded model accuracy: {loaded_accuracy:.4f}\")\n",
        "print(f\"Original model accuracy: {accuracy:.4f}\")\n",
        "print(f\"Models match: {np.array_equal(y_pred_class, loaded_pred)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# Create multi-class dataset\n",
        "X_multi, y_multi = make_classification(\n",
        "    n_samples=1500,\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=5,\n",
        "    n_classes=3,\n",
        "    n_clusters_per_class=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split data\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
        "    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
        ")\n",
        "\n",
        "# Create multi-class classifier\n",
        "xgb_multi = xgb.XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=3,\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "\n",
        "# Train\n",
        "xgb_multi.fit(\n",
        "    X_train_multi, y_train_multi,\n",
        "    eval_set=[(X_test_multi, y_test_multi)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Predictions\n",
        "y_pred_multi = xgb_multi.predict(X_test_multi)\n",
        "y_pred_proba_multi = xgb_multi.predict_proba(X_test_multi)\n",
        "\n",
        "# Evaluate\n",
        "multi_accuracy = accuracy_score(y_test_multi, y_pred_multi)\n",
        "print(f\"Multi-Class Accuracy: {multi_accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_multi, y_pred_multi))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_multi = confusion_matrix(y_test_multi, y_pred_multi)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_multi, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.title('Multi-Class Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Model Ensemble\n",
        "\n",
        "Combine multiple XGBoost models for better performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# Create multiple models with different configurations\n",
        "models_ensemble = []\n",
        "configs = [\n",
        "    {'max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 100},\n",
        "    {'max_depth': 6, 'learning_rate': 0.1, 'n_estimators': 100},\n",
        "    {'max_depth': 9, 'learning_rate': 0.05, 'n_estimators': 200},\n",
        "]\n",
        "\n",
        "for i, config in enumerate(configs):\n",
        "    model = xgb.XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        random_state=42 + i,\n",
        "        eval_metric='logloss',\n",
        "        **config\n",
        "    )\n",
        "    model.fit(X_train_class, y_train_class, eval_set=[(X_test_class, y_test_class)], verbose=False)\n",
        "    models_ensemble.append(model)\n",
        "\n",
        "# Ensemble predictions (voting)\n",
        "predictions = np.array([model.predict(X_test_class) for model in models_ensemble])\n",
        "ensemble_pred = (predictions.mean(axis=0) > 0.5).astype(int)\n",
        "\n",
        "# Evaluate\n",
        "ensemble_accuracy = accuracy_score(y_test_class, ensemble_pred)\n",
        "print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
        "\n",
        "# Individual accuracies\n",
        "print(\"\\nIndividual Model Accuracies:\")\n",
        "for i, model in enumerate(models_ensemble):\n",
        "    pred = model.predict(X_test_class)\n",
        "    acc = accuracy_score(y_test_class, pred)\n",
        "    print(f\"  Model {i+1}: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Feature Engineering\n",
        "\n",
        "Create new features to improve model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project: XGBoost Gradient Boosting\n",
        "# Author: Molla Samser (Founder)\n",
        "# Website: https://rskworld.in\n",
        "\n",
        "# Create DataFrame with features\n",
        "df_features = pd.DataFrame(X_class, columns=feature_names)\n",
        "\n",
        "# Feature engineering\n",
        "df_features['feature_1_squared'] = df_features['feature_1'] ** 2\n",
        "df_features['feature_2_squared'] = df_features['feature_2'] ** 2\n",
        "df_features['feature_1_x_feature_2'] = df_features['feature_1'] * df_features['feature_2']\n",
        "df_features['feature_mean'] = df_features[feature_names].mean(axis=1)\n",
        "df_features['feature_std'] = df_features[feature_names].std(axis=1)\n",
        "\n",
        "# Prepare engineered data\n",
        "X_engineered = df_features.values\n",
        "X_train_eng, X_test_eng, y_train_eng, y_test_eng = train_test_split(\n",
        "    X_engineered, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "# Train model with engineered features\n",
        "xgb_eng = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_eng.fit(X_train_eng, y_train_eng, eval_set=[(X_test_eng, y_test_eng)], verbose=False)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_eng = xgb_eng.predict(X_test_eng)\n",
        "eng_accuracy = accuracy_score(y_test_eng, y_pred_eng)\n",
        "\n",
        "print(f\"Original Features Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Engineered Features Accuracy: {eng_accuracy:.4f}\")\n",
        "print(f\"Improvement: {eng_accuracy - accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated:\n",
        "- ✅ Basic XGBoost classification and regression\n",
        "- ✅ Hyperparameter tuning with GridSearchCV\n",
        "- ✅ Cross-validation techniques\n",
        "- ✅ Feature importance analysis\n",
        "- ✅ Model interpretation with SHAP\n",
        "- ✅ Early stopping\n",
        "- ✅ Model persistence\n",
        "\n",
        "For more advanced techniques and custom implementations, refer to the Python scripts in this project.\n",
        "\n",
        "---\n",
        "\n",
        "**Project Information:**\n",
        "- **Author:** Molla Samser (Founder)\n",
        "- **Designer & Tester:** Rima Khatun\n",
        "- **Website:** https://rskworld.in\n",
        "- **Email:** help@rskworld.in, support@rskworld.in\n",
        "- **Phone:** +91 93305 39277\n",
        "- **Address:** Nutanhat, Mongolkote, Purba Burdwan, West Bengal, India, 713147\n",
        "- **GitHub:** https://github.com/rskworld\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
